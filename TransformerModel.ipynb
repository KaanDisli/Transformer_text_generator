{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "925787ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26b901b6b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "with open(\"Shakespeare_text.txt\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 3e-4\n",
    "text_size = len(text)\n",
    "batch_size = 64\n",
    "context_size = 256\n",
    "dim = 2\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "training_text  = text[:text_size]\n",
    "training_size = len(training_text)\n",
    "val_text = text[training_size:]\n",
    "val_size = len(val_text)\n",
    "embd_size =384\n",
    "head_size = 64\n",
    "n_layer = 6 \n",
    "dropout  = 0.3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b78f7ab-072d-4ced-8864-50d479c885e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[char] for char in s ]\n",
    "decode = lambda s: ''.join([itos[i] for i in s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b02c12a-8ea4-4ad6-87fd-c2f791d25786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1, 39,  1, 51, 53, 52, 58, 46],\n",
       "         [47, 52, 45,  8,  0,  0, 28, 27],\n",
       "         [39, 52,  1, 51, 63,  1, 40, 56],\n",
       "         [ 1, 53, 52, 43,  6,  1, 58, 46]]),\n",
       " tensor([[39,  1, 51, 53, 52, 58, 46,  1],\n",
       "         [52, 45,  8,  0,  0, 28, 27, 24],\n",
       "         [52,  1, 51, 63,  1, 40, 56, 53],\n",
       "         [53, 52, 43,  6,  1, 58, 46, 53]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generates a random batch of characters with (context_size, batch_size)\n",
    "def get_batch():\n",
    "    ix = torch.randint(training_size - context_size, (batch_size,))\n",
    "    x = [training_text[i:i+context_size] for i in ix]\n",
    "    x = [encode(char) for char  in x]\n",
    "    x = torch.tensor(x)\n",
    "    label = [training_text[i+1:i+context_size+1] for i in ix]\n",
    "    label = [encode(char) for char in label ] \n",
    "    label = torch.tensor(label)\n",
    "    return x , label\n",
    "get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918cbfb5-3db4-49db-bcce-bf800ba19b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_batch():\n",
    "    ix = torch.randint(val_size - context_size, (batch_size,))\n",
    "    x = [val_text[i:i+context_size] for i in ix]\n",
    "    x = [encode(char) for char  in x]\n",
    "    x = torch.tensor(x)\n",
    "    label = [val_text[i+1:i+context_size+1] for i in ix]\n",
    "    label = [encode(char) for char in label ] \n",
    "    label = torch.tensor(label)\n",
    "    return x , label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7695f33b-936b-4b95-a490-a569a13887ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model): \n",
    "    X, labels = get_batch() #X: (batch_size , context_size), labels: (batch_size , context_size)\n",
    "    loss_fn = nn.CrossEntropyLoss() #takes (batch_size * context_size, vocab_size) and (batch_size * context_size)\n",
    "    loss = 0\n",
    "    logit_list = []\n",
    "    for idx in X.flatten():\n",
    "        _, logits = model.predict(idx.item()) \n",
    "        logit_list.append(logits.unsqueeze(0)) #we add another dimension so we can concatanate the list into a tensor later more efficiently.  \n",
    "        #Currently a list of (batch_size * context_size , (1,vocabsize))\n",
    "                         \n",
    "    labels = labels.flatten() #(batch_size * context_size)\n",
    "    logit_tensor = torch.cat(logit_list,dim=0)    #The tensors must have the same shape in all dimensions except the one specified by dim. \n",
    "    #All elements in the list are concatanated along the first dimension to produce a tensor  (batch_size * context_size, vocab_size) \n",
    "    loss = loss_fn(logit_tensor,labels)\n",
    "    return loss.item()\n",
    "    \n",
    "            \n",
    "            \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86cb3d07-68a7-4943-be5d-ba7ed6fc0dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing the Bigram Model manually without the Neural Network Module\n",
    "class BigramModelManual(): \n",
    "    def __init__(self, training_text,vocab): \n",
    "        self.text = text\n",
    "        self.vocab = vocab \n",
    "        self.training_text = training_text\n",
    "        self.vocab_size = len(vocab) \n",
    "        self.matrice = np.zeros((self.vocab_size,self.vocab_size)) #Matrix Vocab_Size x Vocab_Size\n",
    "        self.training_size = len(training_text)\n",
    "    \n",
    "    def train(self):  #normalizing the matrice based on the frequency of the following character\n",
    "        for i in range(self.training_size-1):\n",
    "            self.matrice[self.training_text[i],self.training_text[i+1]] +=1            \n",
    "        for row in range(len(self.matrice)):\n",
    "            row_sum = self.matrice[row].sum()\n",
    "            if row_sum > 0:  # Avoid division by zero\n",
    "                    self.matrice[row] /= row_sum\n",
    "\n",
    "    def get_matrice(self):\n",
    "        return self.matrice\n",
    "\n",
    "    def get_logits(self,idx):\n",
    "         torch.tensor(self.matrice[idx])\n",
    "        \n",
    "    def predict(self,idx): #idx is the input character\n",
    "        selected_value = np.random.choice(self.vocab, p=self.matrice[idx])\n",
    "        logits = torch.tensor(self.matrice[idx])\n",
    "        return encode(selected_value)[0], logits\n",
    "    \n",
    "        \n",
    "        \n",
    "    def generate(self,generation_limit,initial_character=' '):\n",
    "        generated_text = []\n",
    "        idx = encode(initial_character)[0]\n",
    "        for i in range(generation_limit):  \n",
    "            pred, logits = self.predict(idx)\n",
    "            generated_text.append(pred)\n",
    "            idx = generated_text[-1]\n",
    "        return decode(generated_text)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dc68f57-fad3-4723-b088-28c688fd2581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st ielold LAngor nd was be,\n",
      "HINGomal ifay thar'sik\n",
      "Loss:  4.044811546435326\n"
     ]
    }
   ],
   "source": [
    "#Generating text with the manual BigramModel\n",
    "\n",
    "model = BigramModelManual(encode(training_text),chars)\n",
    "matrix = model.get_matrice()\n",
    "model.train()\n",
    "print(model.generate(50))\n",
    "print(\"Loss: \" , estimate_loss(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc5b921-8d78-43c2-882d-18a50f3c953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "#BigramModel using the Neural Network module\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self,vocab_size): \n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size) #initializing the embedding table (vocabsize,vocab_size)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        logits = self.token_embedding_table(idx) #For each token in the input tensor idx, the model retrieves its corresponding logits. B,T -> B,T,C\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits_flatten = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits_flatten,targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            #idx = B,T\n",
    "            logits,_  = self.forward(idx) # we get the logits for each Batch (B,T,C\n",
    "            logits = logits[:,-1,:] #convert logits to B,C (taking out the last column of every batch )\n",
    "            proba = F.softmax(logits, dim=-1)  #convert the last dimension to probabilities\n",
    "            new_token= torch.multinomial(proba,num_samples = 1) #generate new token based on proabilities (B,1)\n",
    "            idx = torch.cat((idx,new_token),dim=1)\n",
    "            \n",
    "        return idx\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5638a317-8458-4410-8a35-03485ff07319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.416097402572632\n",
      "\n",
      "PFW'd ABun wicay grear car hyothed:\n",
      "NEThe:\n",
      "\n",
      "\n",
      "Qat hebthergneYUMy;ZAm;' inde g wals slulobus whe\n",
      "\n",
      "KIZ!\n"
     ]
    }
   ],
   "source": [
    "#Trainign the BigramModule weights\n",
    "X, labels = get_batch()\n",
    "m = BigramModel(vocab_size)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype = torch.long)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "#loss, optimizer and logits are connected through nn.Embedding\n",
    "for iteration in range (10000):\n",
    "    X, labels = get_batch() #We get a random batch \n",
    "    logits, loss = m.forward(X,targets=labels) #We calculate the logits obtained from this batch \n",
    "    optimizer.zero_grad(set_to_none=True) #Not sure what this does\n",
    "    loss.backward() #Computes the gradients of the loss\n",
    "    optimizer.step() # Updates the weights of the model\n",
    "print(loss.item())\n",
    "\n",
    "print(decode(m.generate(idx,100)[0].tolist()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c2c410a-b7af-4b6e-a296-66ac70589888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A single self attention head \n",
    "class Head(nn.Module):\n",
    "    def __init__(self,size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.key = nn.Linear(embd_size,size, bias= False) \n",
    "        self.query = nn.Linear(embd_size,size, bias= False)\n",
    "        self.value = nn.Linear(embd_size,size, bias= False)\n",
    "        self.dropout = nn.Dropout(dropout) #we dropout and set some affinities to 0 so that the model doesnt overly really on any one token \n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        k = self.key(x) #we perform linear transformation on x and get key vectors B T 4\n",
    "        q = self.query(x)  #we perform linear transformation on x and get query vectors B T 4\n",
    "        v = self.value(x) #we perform linear transformation on x and get value vectors B T 4\n",
    "        self.weights = q @ k.transpose(-2,-1)*self.size ** -0.5 #we perform matrix multiplication on key and query matrice and scale it down by square root of head_Size do reduce variance for the softmax\n",
    "        #B T 4 @ B 4 T -> B T T For every token this represents the affinity with other tokens in the same context\n",
    "        tril = torch.tril(torch.ones(context_size,context_size))\n",
    "        self.weights = self.weights.masked_fill(tril == 0, float('-inf')) #we perform triangular masking to disregard tokens that are positionally in front of the token we are analysing \n",
    "        self.weights = F.softmax(self.weights, dim= -1)\n",
    "        self.weights = self.dropout(self.weights)\n",
    "        output = self.weights @ v \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "031e4185-efe2-4c3c-8be9-a066568e257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We combine multiple heads into 1 multihead attention block\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,nb_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.nb_heads = nb_heads\n",
    "        self.head_size = head_size\n",
    "        self.Heads = nn.ModuleList([Head(head_size) for i in range(nb_heads) ])\n",
    "        self.proj = nn.Linear(embd_size,embd_size) #Without self.proj, the heads would remain separate and untransformed, which could limit the model's ability to integrate the insights gained from different attention heads.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = torch.cat ( [heads.forward(x) for heads in self.Heads], dim = 2  )\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out #We concatanate all the invidiual heads to form an embd_size mutli_head\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88d808e4-e3d9-4461-802d-997234285a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The outputs from the MHA are fedforward into the next block using an activation function and linear transformation\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embd_size, 4 * embd_size), nn.ReLU(),nn.Linear(4 * embd_size,embd_size), # multiplying by 4 allows to  increase the size of the skip connection pathway?\n",
    "  nn.Dropout(dropout)      ) #We set some features to 0 so that the model doesnt overly really on any single feature of a token\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d5448d1-4a04-48ea-a714-d4e1e2575df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self,nb_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(nb_heads,head_size) \n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln_1 = nn.LayerNorm(embd_size)#We normalize the features of each token among themselves\n",
    "        self.ln_2 = nn.LayerNorm(embd_size) \n",
    "    def forward(self,x):\n",
    "        x = x + self.sa(self.ln_1(x)) #we provide a skip connection so that the gradient does not disappear for earlier layers\n",
    "        return x +  self.ffwd(self.ln_2(x))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c049f36c-2f7a-48b7-a0ce-060620ddb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,embd_size): \n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,embd_size) #initializing the token embedding table for the identity of each token (embd_size,embd_size)\n",
    "        self.positional_embedding_table = nn.Embedding(context_size ,embd_size) #initializing the positional embedding table for each position in the context \n",
    "        self.lm_head = nn.Linear(embd_size,vocab_size)\n",
    "        self.blocks = nn.Sequential( *[Block(head_size,embd_size//head_size) for _ in range(n_layer)] ) \n",
    "        self.ln_f =  nn.LayerNorm(embd_size)\n",
    "    def forward(self,idx,targets=None):\n",
    "        token_embed_value = self.token_embedding_table(idx) \n",
    "        token_positional_value = self.positional_embedding_table(torch.arange(context_size,device=device)) \n",
    "        pos_and_embed_value = token_positional_value + token_embed_value #we sum the identity and the position of the token B , T, embd_size\n",
    "        #4 8 32\n",
    "        x = self.blocks(pos_and_embed_value) #we forward the values \n",
    "        logits = self.lm_head(x) #we transform the output into a class size array to get the logits\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits_flatten = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits_flatten,targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            if context_size > len(idx[0]):\n",
    "                new_token = torch.zeros((1,context_size - len(idx)), dtype = torch.long) #T \n",
    "                idx = torch.cat((idx,new_token),dim=1)\n",
    "                logits,_  = self.forward(idx) \n",
    "            else:\n",
    "                logits,_  = self.forward(idx[: , -context_size:]) # we get the logits for each Batch (B,T,C)\n",
    "            logits = logits[:,-1,:] #convert logits to B,C (taking out the last column of every batch )\n",
    "            proba = F.softmax(logits, dim=-1)  #convert the last dimension to probabilities\n",
    "            new_token= torch.multinomial(proba,num_samples = 1) #generate new token based on proabilities (B,1)\n",
    "            idx = torch.cat((idx,new_token),dim=1)\n",
    "           \n",
    "            \n",
    "        return idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed44402f-9420-4669-b0b5-3cb6d5894e00",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward(X,targets\u001b[38;5;241m=\u001b[39mlabels) \u001b[38;5;66;03m#We calculate the logits obtained from this batch \u001b[39;00m\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#Not sure what this does\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Computes the gradients of the loss\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Updates the weights of the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m = TransformerModel(embd_size)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "#Training\n",
    "#loss, optimizer and logits are connected through nn.Embedding\n",
    "for iteration in range (10000):\n",
    "    X, labels = get_batch() #We get a random batch \n",
    "    logits, loss = m.forward(X,targets=labels) #We calculaote the logits obtained from this batch \n",
    "    optimizer.zero_grad(set_to_none=True) #Not sure what this does\n",
    "    loss.backward() #Computes the gradients of the loss\n",
    "    optimizer.step() # Updates the weights of the model\n",
    "print(\"Training loss\")\n",
    "print(loss.item())\n",
    "\n",
    "\n",
    "#Validation \n",
    "for iteration in range (10000):\n",
    "    X, labels = get_validation_batch() #We get a random batch \n",
    "    logits, loss = m.forward(X,targets=labels) #We calculate the logits obtained from this batch \n",
    "    optimizer.zero_grad(set_to_none=True) #Not sure what this does\n",
    "    loss.backward() #Computes the gradients of the loss\n",
    "    optimizer.step() # Updates the weights of the model\n",
    "print(\"Validation loss\")\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf57641-1334-4f14-9452-5d13b1aa93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text generation\n",
    "idx = torch.zeros((1,1), dtype = torch.long) #Initalize with an empty character\n",
    "print(decode(m.generate(idx,100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99722456-718f-4db9-bf9b-4257b743f004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f5727-3a23-4044-bb79-0ca6164d6949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6172d62-b434-45e7-8bc1-1561214678c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
